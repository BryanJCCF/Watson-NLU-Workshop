{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IBM Workshop - Natural Language Understanding "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta parte del workshop, utilizaremos una instancia de [Watson Natural Language Understanding](https://cloud.ibm.com/catalog/services/natural-language-understanding) para obtener insights de nuestros datos.\n",
    "\n",
    "Watson Natural Language Understanding es un producto nativo de la nube de IBM que utiliza algoritmos de deep learning para extraer metadata de un texto como pueden ser entidades, palabras clave, categorias, sentimientos, emociones, relaciones y sintaxis\n",
    "\n",
    "Existe una [API](https://cloud.ibm.com/apidocs/natural-language-understanding?code=python) que estaremos utilizando junto con [Watson Python SDK](https://github.com/watson-developer-cloud/python-sdk) para el análisis de nuestros datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contenido\n",
    "\n",
    "- [1.0 Configuración - Instalar módulos](#setup)\n",
    "- [2.0 Prueba de la API de NLU](#test)\n",
    "- [3.0 Importar datos y creación de un Dataframe en pandas](#pandas)\n",
    "- [4.0 Limpieza y preparación de datos para el puntaje de la API de NLU](#clean)\n",
    "- [5.0 Análisis de la respuesta del servicio de NLU ](#analyze)\n",
    "- [6.0 Obtener el sentimiento por columna](#sentiment-row)\n",
    "- [7.0 Visualización en una gráfica con matplotlib](#graph)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0 Configuración - Instalar módulos<a name=\"setup\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizaremos el [Watson Python SDK](https://github.com/watson-developer-cloud/python-sdk) para accesar a las [NLU APIs](https://cloud.ibm.com/apidocs/natural-language-understanding?code=python) en este notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade numpy==1.16.6\n",
    "!pip install --upgrade pandas==1.0.5\n",
    "!pip install --upgrade ibm-watson==4.7.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importante: Reinicia el kernel del Jupiter Notebook \n",
    "Reinicia el kernel dando click en el apartado superior llamado `Kernel` y eligiendo la opción `Restart`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importar los módulos de python desde el Watson Python SDKs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from ibm_watson import NaturalLanguageUnderstandingV1\n",
    "from ibm_cloud_sdk_core.authenticators import IAMAuthenticator\n",
    "from ibm_watson.natural_language_understanding_v1 import Features,CategoriesOptions,EmotionOptions,KeywordsOptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Añade las credenciales del servicio de NLU \n",
    "Pega la [IAM API Key](https://cloud.ibm.com/apidocs/natural-language-understanding?code=python#authentication) y el [Service URL](https://cloud.ibm.com/apidocs/natural-language-understanding?code=python#service-endpoint) que obtuviste al crear la instancia de NLU en IBM CLOUD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reemplaza los asteriscos por tu [IAM API Key](https://cloud.ibm.com/apidocs/natural-language-understanding?code=python#authentication)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IAM_KEY = '*******************'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reemplaza los asteriscos por tu [NLU Service URL](https://cloud.ibm.com/apidocs/natural-language-understanding?code=python#service-endpoint)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SERVICE_URL = '*******************'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 Prueba de la API de NLU <a name=\"test\"></a>\n",
    "Vamos a correr una pequeña función para verificar que todo esté funcionando correctamente. Usaremos una [pagina web](https://www.ibm.com) para observar como Watson NLU puede extraer categorías cuando se le da un URL. [Este ejemplo](https://cloud.ibm.com/apidocs/natural-language-understanding?code=python#categories) está mencionado en la documentación de Watson NLU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authenticator = IAMAuthenticator(IAM_KEY)\n",
    "natural_language_understanding = NaturalLanguageUnderstandingV1(version='2020-08-01',authenticator=authenticator)\n",
    "\n",
    "natural_language_understanding.set_service_url(SERVICE_URL)\n",
    "\n",
    "response = natural_language_understanding.analyze(\n",
    "    url='www.ibm.com',\n",
    "    features=Features(categories=CategoriesOptions(limit=3))).get_result()\n",
    "\n",
    "print(json.dumps(response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0 Importar datos y creación de un Dataframe en pandas <a name=\"pandas\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a leer el archivo [cfpciti.csv](https://raw.githubusercontent.com/ibmdevelopermx/Watson-NLU-Workshop/main/Data/cfpbciti.csv) el cual contiene quejas levantadas en el Buró de crédito de consumidores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/ibmdevelopermx/Watson-NLU-Workshop/main/Data/cfpbciti.csv')\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.0 Limpieza y preparación de datos para el puntaje de la API de NLU] <a name=\"clean\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estamos interesados en el sentimiento de el cliente hacia varias cosas como el `Product` o el '`Sub-Product`. La columna llamada `Customer complaint narrative` contiene el texto en el cual nos tenemos que enfocar. Vamos a analizarlo.\n",
    "Las primeras columnas contienen valores 'NaN', así que vamos a ver el valor de la columna 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = df.loc[3,\"Consumer complaint narrative\"]\n",
    "text1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vamos a eliminar todas los valores 'NaN' encontrados en la columna de `Consumer complaint narrative` utilizando un método de Pandas llamado [dropna()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.dropna.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=df['Consumer complaint narrative'].dropna(how = 'all')\n",
    "df2.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a convertir la columna de el dataframe en un string para mandarlo al endpoint del servicio de NLU para evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text = df2.to_string()\n",
    "df_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vamos a mandar estos datos a Watson para obtener [palabras clave (keywords)](https://cloud.ibm.com/apidocs/natural-language-understanding?code=python#keywords), [sentimientos (sentiment)](https://cloud.ibm.com/apidocs/natural-language-understanding?code=python#sentiment) y [emociones (emotions)](https://cloud.ibm.com/apidocs/natural-language-understanding?code=python#emotion) ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = natural_language_understanding.analyze(\n",
    "    text = df_text,\n",
    "    features=Features(keywords=KeywordsOptions(sentiment=True,emotion=True,limit=5))).get_result()\n",
    "\n",
    "print(json.dumps(response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el resultado obtenido se puede observar que se tomaron los campos 'XX/XX' de las fechas que están seleccionadas. Como esto ocurre frecuentemente en este dataset, la respuesta de el servicio marca este valor como algo de alta relevancia. Vamos a eliminar los valores 'XX' para obtener una respuesta mas clara de Watson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 =df2.replace(regex=['X+'],value='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text = df2.to_string()\n",
    "df_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya con esos valores eliminados, vamos a volver a mandar la petición a el servicio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = natural_language_understanding.analyze(\n",
    "    text = df_text,\n",
    "    features=Features(keywords=KeywordsOptions(sentiment=True,emotion=True,limit=5))).get_result()\n",
    "\n",
    "print(json.dumps(response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El servicio arrojó información que podemos utilizar. Se puede observar en la respuesta de el servicio que hay un límite de 50,000 caracteres. Vamos a corregir eso mas adelante, por ahora vamos a analizar los datos obtenidos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.0 Análisis de la respuesta del servicio de NLU  <a name=analyze></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a crear un dataframe con la respuesta de la API. Primero vamos a observar una parte de la respuesta en formato json asociada con la llave `keywords`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "respj = json.dumps(response['keywords'])\n",
    "respj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se ve genial! Ahora vamos a crear un dataframe con ese json utilizando el método [read_json()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_json.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_df = pd.read_json(respj)\n",
    "json_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esto funciona, pero la columna `Sentiment` y `emotion` estan compuestas de un json que contiene múltiples valores en un [diccionario de Python](https://docs.python.org/3/tutorial/datastructures.html#dictionaries).\n",
    "Podemos utilizar la función [json_normalize()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.json_normalize.html) para crear un dataframe que separa las columnas `sentiment`y `emotion`. Vamos a eliminar algúnas columnas de nuestro dataframe para enfocarnos solamente en `sentiment.score` y en `emotion`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_df = pd.json_normalize(response['keywords'])\n",
    "norm_df.drop('relevance',inplace = True, axis = 1)\n",
    "norm_df.drop('count',inplace = True, axis = 1)\n",
    "norm_df.drop('sentiment.label',inplace = True, axis = 1)\n",
    "norm_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta exploración de los datos nos otorga herramientas para trabajar. Vamos a continuar el análisis del texto en las siguientes secciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.0 Obtener el sentimiento por columna <a name=\"sentiment-row\"></a>\n",
    "\n",
    "Ahora, obtengamos información sobre sentimientos y emociones por fila, para proporcionar más granularidad.\n",
    "La cantidad de llamadas a la API que puede realizar a Watson NLU [contiene un limite y depende de tu plan de servicio](https://cloud.ibm.com/catalog/services/natural-language-understanding) (En nuestro caso es el plan lite), asi que para limitar estas llamadas a la API de NLU vamos a empezar con solo 50 filas definiendo el valor `num_rows` en 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rows = df.head(num_rows)\n",
    "df_rows = df_rows.dropna(subset=['Consumer complaint narrative'],how = 'any')\n",
    "df_rows =df_rows.replace(regex=['X+'],value='')\n",
    "df_rows.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notamos que cuando eliminamos las columnas que contenian valores `NaN` en `Customer complaint narrative` los indices de las tablas ya no eran secuenciales. Vamos a utilizar el método [reset_index](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.reset_index.html) para solucionarlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rows.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existen muchas maneras de iterar en las filas de un dataframe, nosotros vamos a utilizar el método [iterrows()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.iterrows.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero, tenemos una fecha para esas entradas, vamos a colocarlas en un formato [Pandas datetime](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html). Podemos utilizar esto mas tarde para hacer graficas en series de tiempo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df_rows.iterrows():\n",
    "    df_rows.loc[index,'Date received'] = datetime.strptime(row['Date received'], \"%m/%d/%y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, vamos a buscar algo que podamos usar con Watson NLU para el análisis de sentimiento de la retroalimentación del cliente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rows.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (df_rows['Consumer complaint narrative'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esto se ve como algo que nos interesa. Ahora vamos a crear una lista para guardar estas `respuestas`, llamar a el servicio de NLU con los datos y llenar esta lista de respuestas. Haremos lo mismo con una lista llamada `normalize` que podemos utilizar junto con el método [json_normalize()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.json_normalize.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = []\n",
    "normalize = []\n",
    "for index, row in df_rows.iterrows():\n",
    "    \n",
    "    response = natural_language_understanding.analyze(\n",
    "    text = row['Consumer complaint narrative'],\n",
    "    features=Features(keywords=KeywordsOptions(sentiment=True,emotion=True,limit=1))).get_result()\n",
    "    normalize.append(pd.json_normalize(response['keywords']))\n",
    "    responses.append(response)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Añadimos la lista de `responses` y la de `normalize`a un df_rows dataframe. Podemos seguir usando estas nuevas características de datos, pero más comúnmente derivaremos nuevos marcos de datos para nuestros experimentos y cambiaremos esos nuevos marcos de datos en su lugar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rows['response'] = responses\n",
    "df_rows.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rows['normalized'] = normalize\n",
    "df_rows.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a crear un nuevo dataframe en donde podamos obtener la columna para la `emocion` `anger` y despues ordenarla por el rating mas alto de `anger`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = df_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in test_df.iterrows():\n",
    "    test_df.loc[index,\"anger\"] = test_df.iloc[index]['response']['keywords'][0]['emotion']['anger']\n",
    "    test_df.loc[index,\"sentiment.score\"] = test_df.iloc[index]['response']['keywords'][0]['sentiment']['score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a ver primero las entradas que tienen mayor rango de `anger`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_df = test_df.sort_values(by='anger', ascending=False)\n",
    "sorted_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, vamos a observar los valores de la columna `Consumer complaint narrative` que causan más enojo (La que está a la cabeza de la lista ordenada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_df.iloc[0]['Consumer complaint narrative']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos hacer lo mismo para otros valores que contengan un rango alto en la lista de enojo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_df.iloc[1]['Consumer complaint narrative']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a observar los valores que contienen el mas alto sentimiento de negatividad. Para esto vamos a ordenar de manera ascendente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_df = test_df.sort_values(by='sentiment.score', ascending=True)\n",
    "sorted_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_df.iloc[0]['Consumer complaint narrative']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bueno, no es sorprendente que la entrada con el sentimiento mas negativo sea la misma que la que tienen mas rating en `anger`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.0 Visualización en una gráfica con matplotlib <a name=\"graph\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a crear algúnas gráficas  utilizando la librería [matplotlib](https://matplotlib.org). Si gustas puedes explorar mas detalles acerca de las magic functions](https://ipython.readthedocs.io/en/stable/interactive/tutorial.html#magics-explained) que se utilizan en los Jupyter notebook que puedes ver con el comando `%matplotlib inline`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Gráfica en series de tiempo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos observar si hay algo interesante cuando ponemos los datos contra el tiempo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_df.plot(kind='line',x='Date received',y='anger',color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sorted_df.plot(kind='line',x='Date received',y='sentiment.score',color='blue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can plot both `sentiment.score` and `anger` against time to look for correlations.\n",
    "Ahora vamos a poner el `sentiment.score` y el `anger` contra el tiempo para buscar alguna correlación. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_df.plot(kind='line',x='Date received',y=['sentiment.score','anger'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Gráficas de Barra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos sumar el número de veces en las que aparece la columna `Product` o `Sub-product` utilizando la funcion de [Python collections library Counter](https://docs.python.org/2/library/collections.html#collections.Counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Después, camos a crear una gráfica de barras para ver a que `Product` se refieren más en las quejas de los clientes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bar_hist = Counter(sorted_df['Product'].replace('\\n', ''))\n",
    "\n",
    "counts = bar_hist.values()\n",
    "letters = bar_hist.keys()\n",
    "\n",
    "# graph data\n",
    "bar_x_locations = np.arange(len(counts))\n",
    "plt.bar(bar_x_locations, counts, align = 'center')\n",
    "plt.xticks(bar_x_locations, letters, rotation=90)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos hacer lo mismo también para `Sub-product`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bar_hist = Counter(sorted_df['Sub-product'].replace('\\n', ''))\n",
    "\n",
    "counts = bar_hist.values()\n",
    "letters = bar_hist.keys()\n",
    "\n",
    "# graph data\n",
    "bar_x_locations = np.arange(len(counts))\n",
    "plt.bar(bar_x_locations, counts, align = 'center')\n",
    "plt.xticks(bar_x_locations, letters, rotation=90)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Gráfico de dispersión"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, vamos a utilizar matplotlib y numpy para generar un gráfico de dispersión en 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import mpl_toolkits.mplot3d.axes3d as axes3d\n",
    "\n",
    "\n",
    "Xuniques, X = np.unique(sorted_df['Sub-product'], return_inverse=True)\n",
    "Yuniques, Y = np.unique(sorted_df['Product'], return_inverse=True)\n",
    "Z= sorted_df['anger']\n",
    "fig = plt.figure(figsize= [15,8])\n",
    "ax = fig.add_subplot(1, 1, 1, projection='3d',autoscale_on=True)\n",
    "ax.scatter(X, Y, Z, s=10, c='b')\n",
    "ax.set(xticks=range(len(Xuniques)), xticklabels=Xuniques,\n",
    "       yticks=range(len(Yuniques)), yticklabels=Yuniques)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y listo! No olvides visitar el [Github de IBM Developer en español](https://github.com/ibmdevelopermx) para encontrar mas temas acerca de Watson y sus API's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
